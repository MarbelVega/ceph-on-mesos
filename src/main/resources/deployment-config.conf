deployment {
  mon {
    count = 0
    cpus = 1
    mem = 256.0

    # # The type of multi-disk volume to use; valid values are root, path, and mount.
    disk_type = root

    # # Size of persistent volume. In the case of diskType = mount, the minimum size of disk to allocate.
    disk = 16
  }

  osd {
    # # Number of OSD instances to spawn
    count = 0

    cpus = 1

    mem = 1024
    # # The type of multi-disk volume to use for the persistent volume; valid values are root, path, and mount.
    disk_type = mount

    # # Size of persistent volume. In the case of diskType = mount, the minimum size of disk to allocate. It is heavily
    # # ill-advised to use anything except mount disks for OSDs.
    disk = 512000

    # # For diskType = mount, don't allocate drives larger than this.

    # disk_max = 1048576

    # # pathConstraint will tell the ceph framework to only allocate persistent mount volumes at a path which FULLY
    # # matches the provided regular expression (I.E. pretend an implicit '^' is added at the beginning of your regex
    # # and a '$' at the end).

    # path_constraint = "/mnt/ssd-.+"
  }

  rgw {
    count = 0
    cpus = 1
    mem = 256

    # # If port is specified then a port resource is not requested, and it is implied that the container is running on a
    # # network where that port is guaranteed to be available
    # port = 80

    # # docker_flags specifies an array of arbitrary launch parameters to specify for the docker container
    docker_args = {
      # network = weave
      # hostname = "cephrgw.weave.local"
    }
  }
}

settings {
  # These settings are transparently inserted into the generated ceph.conf file, where values in the 'auth {}' will be
  # inserted in the corresponding [auth] ceph.conf section.
  auth {
    cephx = true
    cephx_require_signatures = false
    cephx_cluster_require_signatures = true
    cephx_service_require_signatures = false
  }

  global {
    max_open_files = 131072
    osd_pool_default_pg_num = 128
    osd_pool_default_pgp_num = 128
    osd_pool_default_size = 3
    osd_pool_default_min_size = 1

    mon_osd_full_ratio = .95
    mon_osd_nearfull_ratio = .85
  }

  mon {
    mon_osd_down_out_interval = 600
    mon_osd_min_down_reporters = 4
    mon_clock_drift_allowed = .15
    mon_clock_drift_warn_backoff = 30
    mon_osd_report_timeout = 300
  }
  
  osd {
    osd_journal_size = 100

    osd_mon_heartbeat_interval = 30

    # # crush
    pool_default_crush_rule = 0
    osd_crush_update_on_start = true

    # # backend
    osd_objectstore = filestore

    # # performance tuning
    filestore_merge_threshold = 40
    filestore_split_multiple = 8
    osd_op_threads = 8
    filestore_op_threads = 8
    filestore_max_sync_interval = 5
    osd_max_scrubs = 1

    # # recovery tuning
    osd_recovery_max_active = 5
    osd_max_backfills = 2
    osd_recovery_op_priority = 2
    osd_client_op_priority = 63
    osd_recovery_max_chunk = 1048576
    osd_recovery_threads = 1
  }

  client {
    rbd_cache_enabled = true
    rbd_cache_writethrough_until_flush = true
  }

  mds {
    mds_cache_size = 100000
  }
}
